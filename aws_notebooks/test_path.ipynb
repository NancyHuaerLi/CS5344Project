{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaeb1505-4241-4568-97b3-e14ac077c723",
   "metadata": {
    "execution": {
     "iopub.status.idle": "2022-01-01T00:00:00.0Z",
     "shell.execute_reply": "2022-01-01T00:00:00.0Z",
     "shell.execute_reply.started": "2022-01-01T00:00:00.0Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attach to a cluster to execute a cell."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82d228b5-785f-4dfa-a479-02c3ba43f306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T11:00:17.632755Z",
     "iopub.status.busy": "2023-03-14T11:00:17.632434Z",
     "iopub.status.idle": "2023-03-14T11:00:17.977042Z",
     "shell.execute_reply": "2023-03-14T11:00:17.974407Z",
     "shell.execute_reply.started": "2023-03-14T11:00:17.632729Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2e35b228b443249f55f45ab890ac5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_bucket = 's3://cs5344-twitter-project'\n",
    "path = '/{}/*.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f657307-80c2-45b2-a84c-595c9fb3e58b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T10:29:30.430447Z",
     "iopub.status.busy": "2023-03-14T10:29:30.429942Z",
     "iopub.status.idle": "2023-03-14T10:33:09.538301Z",
     "shell.execute_reply": "2023-03-14T10:33:09.536094Z",
     "shell.execute_reply.started": "2023-03-14T10:29:30.430413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2038d56538624642b90f35ec8837ee63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40483882"
     ]
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc5af33c-7a0f-4ccd-a0d2-776ed143487a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T11:00:20.901725Z",
     "iopub.status.busy": "2023-03-14T11:00:20.901251Z",
     "iopub.status.idle": "2023-03-14T11:00:21.340762Z",
     "shell.execute_reply": "2023-03-14T11:00:21.337118Z",
     "shell.execute_reply.started": "2023-03-14T11:00:20.901692Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c22b599095437181ad8861454ff5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder_list = ['20220122', '20220123','20220124','20220125', \n",
    "               '20220126', '20220127','20220128', '20220129',\n",
    "               '20220130', '20220131', '20220201', '20220202',\n",
    "               '20220203', '20220204', '20220205', '20220206',\n",
    "               '20220207', '20220208', '20220209', '20220210',\n",
    "               '20220211', '20220212', '20220213', '20220214',\n",
    "               '20220215', '20220224', '20220225', '20220226', '20220228']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6dec628-2e05-4ea8-b52c-9eedb0632ab2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T11:01:09.760883Z",
     "iopub.status.busy": "2023-03-14T11:01:09.760386Z",
     "iopub.status.idle": "2023-03-14T11:53:15.775895Z",
     "shell.execute_reply": "2023-03-14T11:53:15.774171Z",
     "shell.execute_reply.started": "2023-03-14T11:01:09.760839Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac290628cf6400f90eb9d3b4f47e910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date ://cs5344-twitter-project/20220122/*.json.gz # of tweets analyzed: 54134\n",
      "7\n",
      "date ://cs5344-twitter-project/20220123/*.json.gz # of tweets analyzed: 59885\n",
      "25\n",
      "date ://cs5344-twitter-project/20220124/*.json.gz # of tweets analyzed: 59523\n",
      "23\n",
      "date ://cs5344-twitter-project/20220125/*.json.gz # of tweets analyzed: 52605\n",
      "23\n",
      "date ://cs5344-twitter-project/20220126/*.json.gz # of tweets analyzed: 49515\n",
      "23\n",
      "date ://cs5344-twitter-project/20220127/*.json.gz # of tweets analyzed: 51921\n",
      "23\n",
      "date ://cs5344-twitter-project/20220128/*.json.gz # of tweets analyzed: 61573\n",
      "23\n",
      "date ://cs5344-twitter-project/20220129/*.json.gz # of tweets analyzed: 67831\n",
      "23\n",
      "date ://cs5344-twitter-project/20220130/*.json.gz # of tweets analyzed: 62530\n",
      "23\n",
      "date ://cs5344-twitter-project/20220131/*.json.gz # of tweets analyzed: 53968\n",
      "23\n",
      "date ://cs5344-twitter-project/20220201/*.json.gz # of tweets analyzed: 53106\n",
      "23\n",
      "date ://cs5344-twitter-project/20220202/*.json.gz # of tweets analyzed: 51720\n",
      "23\n",
      "date ://cs5344-twitter-project/20220203/*.json.gz # of tweets analyzed: 52131\n",
      "23\n",
      "date ://cs5344-twitter-project/20220204/*.json.gz # of tweets analyzed: 55808\n",
      "23\n",
      "date ://cs5344-twitter-project/20220205/*.json.gz # of tweets analyzed: 52299\n",
      "23\n",
      "date ://cs5344-twitter-project/20220206/*.json.gz # of tweets analyzed: 50129\n",
      "23\n",
      "date ://cs5344-twitter-project/20220207/*.json.gz # of tweets analyzed: 47399\n",
      "23\n",
      "date ://cs5344-twitter-project/20220208/*.json.gz # of tweets analyzed: 55439\n",
      "23\n",
      "date ://cs5344-twitter-project/20220209/*.json.gz # of tweets analyzed: 63469\n",
      "23\n",
      "date ://cs5344-twitter-project/20220210/*.json.gz # of tweets analyzed: 58503\n",
      "23\n",
      "date ://cs5344-twitter-project/20220211/*.json.gz # of tweets analyzed: 53739\n",
      "23\n",
      "date ://cs5344-twitter-project/20220212/*.json.gz # of tweets analyzed: 54553\n",
      "23\n",
      "date ://cs5344-twitter-project/20220213/*.json.gz # of tweets analyzed: 53164\n",
      "23\n",
      "date ://cs5344-twitter-project/20220214/*.json.gz # of tweets analyzed: 56946\n",
      "23\n",
      "date ://cs5344-twitter-project/20220215/*.json.gz # of tweets analyzed: 54443\n",
      "23\n",
      "date ://cs5344-twitter-project/20220224/*.json.gz # of tweets analyzed: 53035\n",
      "23\n",
      "date ://cs5344-twitter-project/20220225/*.json.gz # of tweets analyzed: 51741\n",
      "23\n",
      "date ://cs5344-twitter-project/20220226/*.json.gz # of tweets analyzed: 50836\n",
      "23\n",
      "date ://cs5344-twitter-project/20220228/*.json.gz # of tweets analyzed: 59555\n",
      "23"
     ]
    }
   ],
   "source": [
    "from py4j.protocol import Py4JJavaError\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import explode, col, udf, concat_ws, from_json, lit, array, expr, size\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "\n",
    "# filter based on the following keywords\n",
    "# test_list = ['russia', 'ukraine', 'putin', 'zelensky'\n",
    "#                                            'russian', 'ukrainian', 'keiv', 'kyiv']\n",
    "test_list = ['nintendo', 'pokemon', 'video game', 'game', 'pokémon legends: arceus', 'pokémon', 'legend of arceus',\n",
    "             'legend arceus', 'legends of arceus', 'pokemon legends: arceus', 'arceus']\n",
    "# regex for filter\n",
    "regex_pattern = \"(?i)\" + \"|\".join(test_list)\n",
    "regex_pattern_hashtag = \"(?i)\" + \"|\".join([x.strip(' ') for x in test_list])\n",
    "\n",
    "\n",
    "# raw = spark.read.json(\"./sample_data/*.json\", allowBackslashEscapingAnyCharacter=True)\n",
    "# raw = spark.read.json(\"./202202*/*.json\", allowBackslashEscapingAnyCharacter=True)\n",
    "def pre_process(folder):\n",
    "    raw = spark.read.json(folder, allowBackslashEscapingAnyCharacter=True)\n",
    "    # twitter json counts\n",
    "    # print(raw.count())\n",
    "    '''\n",
    "        Step 1:\n",
    "        Save all tweets, include original tweets in the dataset and retweeted / quoted tweets\n",
    "\n",
    "        only select necessary columns\n",
    "        can refer to twitter API for better understanding:\n",
    "        https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet\n",
    "    '''\n",
    "\n",
    "    twitter_df = raw.select(\"created_at\", \"id_str\", col(\"user.id_str\").alias(\"user_id_str\"),\n",
    "                            col(\"user.screen_name\").alias(\"user_twitter_handle\"),\n",
    "                            \"in_reply_to_status_id_str\", \"in_reply_to_user_id_str\", \"in_reply_to_screen_name\",\n",
    "                            \"retweeted_status\",\n",
    "                            \"quoted_status\",\n",
    "                            \"text\", col(\"entities.hashtags.text\").alias(\"hashtags\"),\n",
    "                            col(\"quoted_status.created_at\").alias(\"quoted_time\"),\n",
    "                            col(\"quoted_status.id_str\").alias(\"quoted_original_tweet_id_str\"),\n",
    "                            col(\"quoted_status.user.id_str\").alias(\"quoted_original_user_id_str\"),\n",
    "                            col(\"quoted_status.user.screen_name\").alias(\"quoted_original_user_twitter_handle\"),\n",
    "                            col(\"retweeted_status.created_at\").alias(\"retweeted_time\"),\n",
    "                            col(\"retweeted_status.id_str\").alias(\"retweeted_original_tweet_id_str\"),\n",
    "                            col(\"retweeted_status.user.screen_name\").alias(\"retweeted_original_user_twitter_handle\"),\n",
    "                            col(\"retweeted_status.user.id_str\").alias(\"retweeted_original_user_id_str\")\n",
    "                            )\n",
    "\n",
    "    # add all retweeted / quoted original tweet themselves to the twitter df\n",
    "    retweet_rdd = twitter_df.filter(twitter_df.retweeted_status.isNotNull()).select('retweeted_status.*')\n",
    "    quoted_rdd = twitter_df.filter(twitter_df.quoted_status.isNotNull()).select('quoted_status.*')\n",
    "\n",
    "    # select columns\n",
    "    retweet_rdd = retweet_rdd.select(\"created_at\", \"id_str\",\n",
    "                                     col(\"user.id_str\").alias(\"user_id_str\"),\n",
    "                                     col(\"user.screen_name\").alias(\"user_twitter_handle\"),\n",
    "                                     \"in_reply_to_status_id_str\", \"in_reply_to_user_id_str\", \"in_reply_to_screen_name\",\n",
    "                                     # \"quoted_status\",\n",
    "                                     \"text\",\n",
    "                                     col(\"entities.hashtags.text\").alias(\"hashtags\"),\n",
    "                                     col(\"quoted_status.created_at\").alias(\"quoted_time\"),\n",
    "                                     col(\"quoted_status.id_str\").alias(\"quoted_original_tweet_id_str\"),\n",
    "                                     col(\"quoted_status.user.id_str\").alias(\"quoted_original_user_id_str\"),\n",
    "                                     col(\"quoted_status.user.screen_name\").alias(\"quoted_original_user_twitter_handle\")\n",
    "                                     )\n",
    "\n",
    "    quoted_rdd = quoted_rdd.select(\"created_at\", \"id_str\", col(\"user.id_str\").alias(\"user_id_str\"),\n",
    "                                   \"in_reply_to_status_id_str\", \"in_reply_to_user_id_str\", \"in_reply_to_screen_name\",\n",
    "                                   \"text\", col(\"entities.hashtags.text\").alias(\"hashtags\")\n",
    "                                   )\n",
    "    twitter_df = twitter_df.drop(\"retweeted_status\", \"quoted_status\")\n",
    "\n",
    "    # add missing columns to make sure all columns match in the above 3 DFs\n",
    "    for c in retweet_rdd.columns:\n",
    "        if c not in twitter_df.columns:\n",
    "            twitter_df = twitter_df.withColumn(c, lit(None))\n",
    "        if c not in quoted_rdd.columns:\n",
    "            quoted_rdd = quoted_rdd.withColumn(c, lit(None))\n",
    "    for c in quoted_rdd.columns:\n",
    "        if c not in twitter_df.columns:\n",
    "            twitter_df = twitter_df.withColumn(c, lit(None))\n",
    "        if c not in retweet_rdd.columns:\n",
    "            retweet_rdd = retweet_rdd.withColumn(c, lit(None))\n",
    "\n",
    "    for c in twitter_df.columns:\n",
    "        if c not in quoted_rdd.columns:\n",
    "            quoted_rdd = quoted_rdd.withColumn(c, lit(None))\n",
    "        if c not in retweet_rdd.columns:\n",
    "            retweet_rdd = retweet_rdd.withColumn(c, lit(None))\n",
    "\n",
    "    # final twitter DFs\n",
    "    combined_raw = twitter_df.unionByName(retweet_rdd).unionByName(quoted_rdd)\n",
    "    del retweet_rdd\n",
    "    del quoted_rdd\n",
    "    del twitter_df\n",
    "    gc.collect()\n",
    "\n",
    "    # convert hashtag column (array type) to str for regex expression filter\n",
    "    raw_df = combined_raw.withColumn(\"text_hashtag\", concat_ws(\",\", col(\"hashtags\")))\n",
    "\n",
    "    filter_df = raw_df.filter(raw_df.text.rlike(regex_pattern) | raw_df.text_hashtag.rlike(regex_pattern)).distinct()\n",
    "    del raw_df\n",
    "    del combined_raw\n",
    "    gc.collect()\n",
    "    print(\"date \" + folder[2:] + \" # of tweets analyzed: \" + str(filter_df.count()))\n",
    "    return filter_df\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "filter_df = None\n",
    "for f in folder_list:\n",
    "    # if folder[0:7] == './20220':\n",
    "    folder = input_bucket + path.format(f)\n",
    "    try:\n",
    "        partial_df = pre_process(folder)\n",
    "        if cnt == 0:\n",
    "            filter_df = partial_df\n",
    "        else:\n",
    "            filter_df = filter_df.unionByName(partial_df)\n",
    "        del partial_df\n",
    "        gc.collect()\n",
    "    except Py4JJavaError:\n",
    "        print(folder+\" failed\")\n",
    "    cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "862bbbed-c11a-4e5d-83e8-b811fb86c0b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T11:53:15.779720Z",
     "iopub.status.busy": "2023-03-14T11:53:15.778515Z",
     "iopub.status.idle": "2023-03-14T13:35:07.539428Z",
     "shell.execute_reply": "2023-03-14T13:35:07.538031Z",
     "shell.execute_reply.started": "2023-03-14T11:53:15.779676Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93c461982f448aeb9b9270c7274597c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id name done"
     ]
    }
   ],
   "source": [
    "output_bucket = 's3://cs5344-twitter-project/input/'\n",
    "id_name = filter_df.select('user_id_str', 'user_twitter_handle') \\\n",
    "    .union(filter_df.select('in_reply_to_user_id_str', 'in_reply_to_screen_name')).na.drop().distinct()\n",
    "id_name.write.option(\"header\", True).mode('overwrite').csv(output_bucket+'id_name_dict_aws')\n",
    "print('id name done')\n",
    "del id_name\n",
    "gc.collect()\n",
    "\n",
    "filter_df = filter_df.drop(\"user_twitter_handle\", \"in_reply_to_screen_name\",\n",
    "                           \"quoted_original_user_twitter_handle\", \"retweeted_original_user_twitter_handle\")\n",
    "\n",
    "'''\n",
    "    Step 2:\n",
    "    Find all interacted (reply, retweet, quote) users.\n",
    "    if a user A retweet user B's tweet, there will be a directional edge between A and B, A -> B.\n",
    "'''\n",
    "\n",
    "filter_df = filter_df.withColumn(\"connected_user\",\n",
    "                                 array(filter_df.quoted_original_user_id_str, filter_df.in_reply_to_user_id_str,\n",
    "                                       filter_df.retweeted_original_user_id_str))\n",
    "filter_df = filter_df.withColumn(\"connected_user_clean\", expr('filter(connected_user, x -> x is not null)')).drop(\n",
    "    \"connected_user\")\n",
    "\n",
    "\n",
    "filter_df = filter_df.filter(size(filter_df.connected_user_clean) > 0)\n",
    "\n",
    "# each line make sure have only one user and one connected user\n",
    "rdd = filter_df.withColumn(\"connected_user_single\", explode(filter_df.connected_user_clean)).rdd\n",
    "# rdd.toDF().write.mode('overwrite').csv('intermediate_rdd')\n",
    "# del filter_df\n",
    "# gc.collect()\n",
    "\n",
    "# for each user how many times retweet/replay/quote other tweets\n",
    "user_interact_rdd = rdd.map(lambda x: (x[2], 1))\n",
    "user_interact_rdd = user_interact_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# each pair of users (A, B), how many times A retweet this person B\n",
    "user_pair_rdd = rdd.map(lambda x: ((x[2], x[15]), 1)).reduceByKey(lambda a, b: a + b)\n",
    "user_pair_rdd = user_pair_rdd.map(lambda x: (x[0][0], x[0][1], x[1]))\n",
    "\n",
    "user_pair_rdd.toDF().write.mode('overwrite').csv(output_bucket+'user_pair_aws') #coalesce(1, shuffle = True).\n",
    "user_interact_rdd.toDF().write.mode('overwrite').csv(output_bucket+'user_interact_aws') #coalesce(1, shuffle = True).\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4265cf-571b-499f-8b73-df82cc086e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
